---
title: "Dropout Layer"
metaTitle: "Dropout Layer | Regularizers | NeuralPy Deep Learning Library"
description: "Applies the Dropout layer to the input tensor."
---
# Dropout
```python
neuralpy.regularizers.Dropout(p=0.5, name=None)
```
Applies the Dropout layer to the input tensor.

The Dropout layer randomly sets input units to 0 with a frequency of rate of `p` at each step during training time. It helps prevent overfitting.

For more information, check [this](https://pytorch.org/docs/stable/nn.html#dropout) page

###  Supported Arguments
- `p=0.5`: (Float) Probability of an element to be zeroed. The value should be between 0.0 and 1.0.
- `name=None`: (String) Name of the layer, if not provided then automatically calculates a unique name for the layer

### Example Code
```python
from neuralpy.models import Sequential
from neuralpy.layers import Dense
from neuralpy.activation_functions import LeakyReLU, Sigmoid
from neuralpy.regularizers import Dropout

# Initializing the Sequential models
model = Sequential()

# Adding layers to the model
model.add(Dense(n_nodes=3, n_inputs=5, bias=True))
model.add(LeakyReLU())
model.add(Dropout())

model.add(Dense(n_nodes=20, bias=True))
model.add(LeakyReLU())
model.add(Dropout())

model.add(Dense(n_nodes=1, bias=True))
model.add(Sigmoid())
```