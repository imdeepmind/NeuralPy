---
id: dropout
title: Dropout
sidebar_label: Dropout
slug: /regularizers/dropout
description: Applies the Dropout layer to the input tensor
image: https://user-images.githubusercontent.com/34741145/81591141-99752900-93d9-11ea-9ef6-cc2c68daaa19.png
hide_title: true
---

# Dropout

```python
neuralpy.layers.regularizers.Dropout(p=0.5, name=None)
```

:::info

Dropout is mostly stable and can be used for any project. In the future, any chance of breaking changes is very low.

:::

Applies the Dropout layer to the input tensor.

The Dropout layer randomly sets input units to 0 with a frequency of rate of `p` at each step during training time. It helps prevent overfitting.

For more information, check [this](https://pytorch.org/docs/stable/nn.html#dropout) page

## Supported Arguments

- `p=0.5`: (Float) Probability of an element to be zeroed. The value should be between 0.0 and 1.0.
- `name=None`: (String) Name of the layer, if not provided then automatically calculates a unique name for the layer

## Example Code

```python
from neuralpy.models import Sequential
from neuralpy.layers.linear import Dense
from neuralpy.layers.activation_functions import LeakyReLU, Sigmoid
from neuralpy.layers.regularizers import Dropout

# Initializing the Sequential models
model = Sequential()

# Adding layers to the model
model.add(Dense(n_nodes=3, n_inputs=5, bias=True))
model.add(LeakyReLU())
model.add(Dropout())

model.add(Dense(n_nodes=20, bias=True))
model.add(LeakyReLU())
model.add(Dropout())

model.add(Dense(n_nodes=1, bias=True))
model.add(Sigmoid())
```
