---
id: relu
title: Leaky ReLU
sidebar_label: ReLU
slug: /activation-functions/relu
description: Applies the rectified linear unit function element-wise
image: https://user-images.githubusercontent.com/34741145/81591141-99752900-93d9-11ea-9ef6-cc2c68daaa19.png
hide_title: true
---

# ReLU

```python
neuralpy.layers.activation_functions.ReLU(name=None)
```

:::info

ReLU Activation Function is mostly stable and can be used for any project. In the future, any chance of breaking changes is very low.

:::

Applies the rectified linear unit function element-wise: ReLU(x) = max(0,x).

To learn more about ReLU, please check PyTorch [documentation](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)

## Supported Arguments

- `name=None`: (String) Name of the activation function layer, if not provided then automatically calculates a unique name for the layer.

## Example Code

```python
from neuralpy.models import Sequential
from neuralpy.layers.linear import Dense
from neuralpy.layers.activation_functions import ReLU

# Making the model
model = Sequential()
model.add(Dense(n_nodes=1, n_inputs=1, bias=True, name="Input Layer"))
model.add(ReLU(name="reluActivation"))
```
