---
title: "Activation Functions"
metaTitle: "Activation Functions| NeuralPy Deep Learning Library"
metaDescription: "Documentation for all activation functions available in NeuralPy Deep Learning Library"
---

# Activation Functions

```python
neuralpy.activation_functions
```

Activation functions are mathematical equations that determine the output of a neural network. The function is attached to each neuron in the network, and determines whether it should be activated (“fired”) or not, based on whether each neuron’s input is relevant for the model’s prediction.

## Supported Activation Functions

- [Softmax](/docs/activation_functions/softmax)
- [Sigmoid](/docs/activation_functions/sigmoid)
- [Tanh](/docs/activation_functions/tanh)
- [ReLU](/docs/activation_functions/relu)
- [Leaky ReLU](/docs/activation_functions/leaky_relu)
- [SELU](/docs/activation_functions/selu)
- [GELU](/docs/activation_functions/gelu)
