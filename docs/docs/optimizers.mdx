---
title: "Opimizers"
metaTitle: "Opimizers | NeuralPy Deep Learning Library"
metaDescription: "Documentation for all Opimizers available in NeuralPy Deep Learning Library"
---

# Opimizers

```python
neuralpy.optimizer
```

Optimizers tie together the loss function and model parameters by updating the model in response to the output of the loss function

NeuralPy currently supports 5 types of Opimizers, Adagrad, Adam, RMSProp, RProp and SGD.

## Supported Opimizers

- Opimizers
  - [Adagrad](/docs/optimizers/adagrad)
  - [Adam](/docs/optimizers/adam)
  - [RMSProp](/docs/optimizers/rmsprop)
  - [RProp](/docs/optimizers/rprop)
  - [SGD](/docs/optimizers/sgd)