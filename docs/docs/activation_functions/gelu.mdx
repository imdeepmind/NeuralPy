---
title: "GELU"
metaTitle: "GELU | Activation Functions | NeuralPy Deep Learning Library"
metaDescription: "Applies the Gaussian Error Linear Units function: GELU(x) = x * Φ(x). where Φ(x) is the Cumulative Distribution Function for Gaussian Distribution."
---

# GELU

```python
neuralpy.activation_functions.GELU(name=None)
```

Applies the Gaussian Error Linear Units function: GELU(x) = x \* Φ(x). where Φ(x) is the Cumulative Distribution Function for Gaussian Distribution.

To learn more about GELU, please check pytorch [documentation](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html#torch.nn.GELU)

## Supported Arguments

- `name`: (String) Name of the activation function layer, if not provided then automatically calculates a unique name for the layer.

## Example Code

```python
from neuralpy.models import Sequential
from neuralpy.layers import Dense
from neuralpy.activation_functions import GELU

# Making the model
model = Sequential()
model.add(Dense(n_nodes=1, n_inputs=1, bias=True, name="Input Layer"))
model.add(GELU())
```
