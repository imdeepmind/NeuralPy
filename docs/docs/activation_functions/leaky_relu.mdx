---
title: "Leaky_ReLU"
metaTitle: "Leaky_ReLU | Activation Functions | NeuralPy Deep Learning Library"
metaDescription: "Applies the element-wise function: Leaky_ReLU(x) = max(0,x)+negative_slope∗min(0,x)."
---

# Leaky_ReLU

```python
neuralpy.activation_functions.Leaky_ReLU(negative_slope=0.01, name=None)
```

Applies the element-wise function: Leaky_ReLU(x) = max(0,x)+negative_slope∗min(0,x).

To learn more about Leaky_ReLU, please check pytorch [documentation](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU)

## Supported Arguments

- `negative_slope`: (Integer) A negative slope for the LeakyReLU
- `name`: (String) Name of the activation function layer, if not provided then automatically calculates a unique name for the layer.

## Example Code

```python
from neuralpy.models import Sequential
from neuralpy.layers import Dense
from neuralpy.activation_functions import Leaky_ReLU

# Making the model
model = Sequential()
model.add(Dense(n_nodes=1, n_inputs=1, bias=True, name="Input Layer"))
model.add(Leaky_ReLU())
```
